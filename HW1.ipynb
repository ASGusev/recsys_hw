{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Матричные факторизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе вам предстоит познакомиться с практической стороной матричных разложений.\n",
    "Работа поделена на 4 задания:\n",
    "1. Вам необходимо реализовать SVD разложения используя SGD на explicit данных\n",
    "2. Вам необходимо реализовать матричное разложения используя ALS на implicit данных\n",
    "3. Вам необходимо реализовать матричное разложения используя BPR(pair-wise loss) на implicit данных\n",
    "4. Вам необходимо реализовать матричное разложения используя WARP(list-wise loss) на implicit данных\n",
    "\n",
    "Мягкий дедлайн 28 Сентября (пишутся замечания, выставляется оценка, есть возможность исправить до жесткого дедлайна)\n",
    "\n",
    "Жесткий дедлайн 5 Октября (Итоговая проверка)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import implicit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# from lightfm.datasets import fetch_movielens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе мы будем работать с explicit датасетом movieLens, в котором представленны пары user_id movie_id и rating выставленный пользователем фильму\n",
    "\n",
    "Скачать датасет можно по ссылке https://grouplens.org/datasets/movielens/1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('ml-1m/ratings.dat', delimiter='::', header=None, \n",
    "        names=['user_id', 'movie_id', 'rating', 'timestamp'], \n",
    "        usecols=['user_id', 'movie_id', 'rating'], engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_info = pd.read_csv('ml-1m/movies.dat', delimiter='::', header=None, \n",
    "        names=['movie_id', 'name', 'category'], engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1197</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1287</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2804</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>594</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>919</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating\n",
       "0        1      1193       5\n",
       "1        1       661       3\n",
       "2        1       914       3\n",
       "3        1      3408       4\n",
       "4        1      2355       5\n",
       "5        1      1197       3\n",
       "6        1      1287       5\n",
       "7        1      2804       5\n",
       "8        1       594       4\n",
       "9        1       919       4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы преобразовать текущий датасет в Implicit, давайте считать что позитивная оценка это оценка >=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit_ratings = ratings.loc[(ratings['rating'] >= 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1287</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2804</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>594</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>919</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>595</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>938</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2398</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  movie_id  rating\n",
       "0         1      1193       5\n",
       "3         1      3408       4\n",
       "4         1      2355       5\n",
       "6         1      1287       5\n",
       "7         1      2804       5\n",
       "8         1       594       4\n",
       "9         1       919       4\n",
       "10        1       595       5\n",
       "11        1       938       4\n",
       "12        1      2398       4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit_ratings.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удобнее работать с sparse матричками, давайте преобразуем DataFrame в CSR матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = implicit_ratings[\"user_id\"]\n",
    "movies = implicit_ratings[\"movie_id\"]\n",
    "user_item = sp.coo_matrix((np.ones_like(users), (users, movies)))\n",
    "user_item_t_csr = user_item.T.tocsr()\n",
    "user_item_csr = user_item.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве примера воспользуемся ALS разложением из библиотеки implicit\n",
    "\n",
    "Зададим размерность латентного пространства равным 64, это же определяет размер user/item эмбедингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n"
     ]
    }
   ],
   "source": [
    "model = implicit.als.AlternatingLeastSquares(factors=64, iterations=100, calculate_training_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве loss здесь всеми любимый RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05be80bcf02a4909aa3cd7842b6e3665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(user_item_t_csr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим похожие фильмы по 1 movie_id = Истории игрушек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                                name                      category\n",
       "0         1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2         3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3         4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4         5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similars = lambda item_id, model : [movie_info[movie_info[\"movie_id\"] == x[0]][\"name\"].to_string() \n",
    "                                        for x in model.similar_items(item_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, симилары действительно оказались симиларами.\n",
    "\n",
    "Качество симиларов часто является хорошим способом проверить качество алгоритмов.\n",
    "\n",
    "P.S. Если хочется поглубже разобраться в том как разные алгоритмы формируют разные латентные пространства, рекомендую загружать полученные вектора в tensorBoard и смотреть на сформированное пространство"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0    Toy Story (1995)',\n",
       " '3045    Toy Story 2 (1999)',\n",
       " \"2286    Bug's Life, A (1998)\",\n",
       " '33    Babe (1995)',\n",
       " '584    Aladdin (1992)',\n",
       " '2315    Babe: Pig in the City (1998)',\n",
       " '360    Lion King, The (1994)',\n",
       " '1838    Mulan (1998)',\n",
       " '1526    Hercules (1997)',\n",
       " '2618    Tarzan (1999)']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similars(1, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь построим рекомендации для юзеров\n",
    "\n",
    "Как мы видим юзеру нравится фантастика, значит и в рекомендациях ожидаем увидеть фантастику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_user_history = lambda user_id, implicit_ratings : [movie_info[movie_info[\"movie_id\"] == x][\"name\"].to_string() \n",
    "                                            for x in implicit_ratings[implicit_ratings[\"user_id\"] == user_id][\"movie_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3399    Hustler, The (1961)',\n",
       " '2882    Fistful of Dollars, A (1964)',\n",
       " '1196    Alien (1979)',\n",
       " '1023    Die Hard (1988)',\n",
       " '257    Star Wars: Episode IV - A New Hope (1977)',\n",
       " '1959    Saving Private Ryan (1998)',\n",
       " '476    Jurassic Park (1993)',\n",
       " '1180    Raiders of the Lost Ark (1981)',\n",
       " '1885    Rocky (1976)',\n",
       " '1081    E.T. the Extra-Terrestrial (1982)',\n",
       " '3349    Thelma & Louise (1991)',\n",
       " '3633    Mad Max (1979)',\n",
       " '2297    King Kong (1933)',\n",
       " '1366    Jaws (1975)',\n",
       " '1183    Good, The Bad and The Ugly, The (1966)',\n",
       " '2623    Run Lola Run (Lola rennt) (1998)',\n",
       " '2878    Goldfinger (1964)',\n",
       " '1220    Terminator, The (1984)']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_user_history(4, implicit_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось! \n",
    "\n",
    "Мы действительно порекомендовали пользователю фантастику и боевики, более того встречаются продолжения тех фильмов, которые он высоко оценил"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations = lambda user_id, model : [movie_info[movie_info[\"movie_id\"] == x[0]][\"name\"].to_string() \n",
    "                                               for x in model.recommend(user_id, user_item_csr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['585    Terminator 2: Judgment Day (1991)',\n",
       " '1271    Indiana Jones and the Last Crusade (1989)',\n",
       " '1182    Aliens (1986)',\n",
       " '1284    Butch Cassidy and the Sundance Kid (1969)',\n",
       " '2502    Matrix, The (1999)',\n",
       " '1178    Star Wars: Episode V - The Empire Strikes Back...',\n",
       " '3402    Close Encounters of the Third Kind (1977)',\n",
       " '847    Godfather, The (1972)',\n",
       " '1892    Rain Man (1988)',\n",
       " '1179    Princess Bride, The (1987)']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations(4, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь ваша очередь реализовать самые популярные алгоритмы матричных разложений\n",
    "\n",
    "Что будет оцениваться:\n",
    "1. Корректность алгоритма\n",
    "2. Качество получившихся симиларов\n",
    "3. Качество итоговых рекомендаций для юзера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Не использую готовые решения, реализовать SVD разложение используя SGD на explicit данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_USERS = users.max() + 1\n",
    "N_ITEMS = movies.max() + 1\n",
    "\n",
    "\n",
    "def scalar_prods(vecs1, vecs2):\n",
    "    return np.sum(vecs1 * vecs2, axis=1).flatten()\n",
    "\n",
    "\n",
    "class MatrixFactorizationBase:\n",
    "    def __init__(self, dim, reg_param, n_users, n_items):\n",
    "        self.dim = dim\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        init_std = 1 / dim ** .5\n",
    "        self.users_embeddings = np.random.normal(0, init_std, (n_users, dim))\n",
    "        self.items_embeddings = np.random.normal(0, init_std, (n_items, dim))\n",
    "        self.users_biases = np.random.uniform(0, .5, n_users)\n",
    "        self.items_biases = np.random.uniform(0, .5, n_items)\n",
    "        self.reg_param = reg_param\n",
    "    \n",
    "    def fit(self, interactions, n_epochs, lr):\n",
    "        pass\n",
    "    \n",
    "    def similarities(self, users_ids, items_ids):\n",
    "        return self.users_biases[users_ids] + self.items_biases[items_ids] + \\\n",
    "                scalar_prods(self.users_embeddings[users_ids], self.items_embeddings[items_ids])\n",
    "    \n",
    "    def recommend(self, user_id, _ = None, n_recs = 20):\n",
    "        similarities = self.items_embeddings @ self.users_embeddings[user_id]\n",
    "        closest_item_ids = similarities.argsort()[::-1][:n_recs]\n",
    "        return list(zip(closest_item_ids, similarities[closest_item_ids]))\n",
    "    \n",
    "    def similar_items(self, item_id, n_items = 20):\n",
    "        similarities = self.items_embeddings @ self.items_embeddings[item_id]\n",
    "        items_by_similariry = similarities.argsort()[::-1]\n",
    "        items_by_similariry = items_by_similariry[items_by_similariry != item_id]\n",
    "        most_similar_items = items_by_similariry[:n_items]\n",
    "        return list(zip(most_similar_items, similarities[most_similar_items]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_BATCH_SIZE = 512\n",
    "\n",
    "\n",
    "class GradientDescentMatrixFactorization(MatrixFactorizationBase):\n",
    "    def __init__(self, dim, reg_alpha, n_users, n_items):\n",
    "        super().__init__(dim, reg_alpha, n_users, n_items)\n",
    "    \n",
    "    \n",
    "    def make_gd_step(self, users_ids, items_ids, targets, lr):\n",
    "        users_gradients = np.zeros_like(self.users_embeddings)\n",
    "        items_gradients = np.zeros_like(self.items_embeddings)\n",
    "        users_biases_gradients = np.zeros_like(self.users_biases)\n",
    "        items_biases_gradients = np.zeros_like(self.items_biases)\n",
    "        \n",
    "        predictions = self.similarities(users_ids, items_ids)\n",
    "        errors_gradients = np.expand_dims(2 * (predictions - targets), 1)\n",
    "        np.add.at(users_gradients, users_ids, errors_gradients * self.items_embeddings[items_ids])\n",
    "        np.add.at(items_gradients, items_ids, errors_gradients * self.users_embeddings[users_ids])\n",
    "        np.add.at(users_gradients, users_ids, 2 * self.reg_param * self.users_embeddings[users_ids])\n",
    "        np.add.at(items_gradients, items_ids, 2 * self.reg_param * self.items_embeddings[items_ids])\n",
    "        np.add.at(users_biases_gradients, users_ids, errors_gradients.flatten())\n",
    "        np.add.at(items_biases_gradients, items_ids, errors_gradients.flatten())\n",
    "        loss = np.sum((predictions - targets) ** 2) + \\\n",
    "               self.reg_param * (np.linalg.norm(self.users_embeddings[users_ids], axis=1).sum() + \\\n",
    "                                 np.linalg.norm(self.items_embeddings[items_ids], axis=1).sum())\n",
    "\n",
    "        self.users_embeddings -= lr * users_gradients\n",
    "        self.items_embeddings -= lr * items_gradients\n",
    "        self.users_biases -= lr * users_biases_gradients\n",
    "        self.items_biases -= lr * items_biases_gradients\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def fit(self, interactions, n_epochs, lr):\n",
    "        n_negatives = n_samples = len(interactions.data)\n",
    "        users_ids = interactions.row\n",
    "        items_ids = interactions.col\n",
    "        \n",
    "        unique_users = np.array(list(set(users_ids)))\n",
    "        unique_items = np.array(list(set(items_ids)))\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            neg_users = np.random.choice(self.n_users, n_negatives)\n",
    "            neg_items = np.random.choice(self.n_items, n_negatives)\n",
    "            all_users = np.concatenate((users_ids, neg_users))\n",
    "            all_items = np.concatenate((items_ids, neg_items))\n",
    "            targets = np.concatenate((np.ones(n_samples), np.zeros(n_negatives)))\n",
    "            indexes = np.arange(n_samples + n_negatives)\n",
    "            np.random.shuffle(indexes)\n",
    "            \n",
    "            loss = 0.\n",
    "            for batch_start in range(0, len(indexes), SGD_BATCH_SIZE):\n",
    "                batch_indexes = indexes[batch_start:batch_start + SGD_BATCH_SIZE]\n",
    "                loss += self.make_gd_step(all_users[batch_indexes], all_items[batch_indexes], \n",
    "                                          targets[batch_indexes], lr)\n",
    "            \n",
    "            print(f'Epoch {epoch + 1} loss {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss 207739.255\n",
      "Epoch 2 loss 170673.102\n",
      "Epoch 3 loss 161184.963\n",
      "Epoch 4 loss 158644.071\n",
      "Epoch 5 loss 157408.196\n",
      "Epoch 6 loss 156283.177\n",
      "Epoch 7 loss 155980.688\n",
      "Epoch 8 loss 155240.273\n",
      "Epoch 9 loss 155232.102\n",
      "Epoch 10 loss 154799.076\n",
      "Epoch 1 loss 120608.357\n",
      "Epoch 2 loss 111983.791\n",
      "Epoch 3 loss 108809.027\n",
      "Epoch 4 loss 106787.285\n",
      "Epoch 5 loss 105632.541\n",
      "Epoch 6 loss 104470.472\n",
      "Epoch 7 loss 104048.391\n",
      "Epoch 8 loss 103079.217\n",
      "Epoch 9 loss 102889.211\n",
      "Epoch 10 loss 102215.109\n",
      "Epoch 1 loss 100178.767\n",
      "Epoch 2 loss 99653.408\n",
      "Epoch 3 loss 99449.168\n",
      "Epoch 4 loss 99181.710\n",
      "Epoch 5 loss 99206.868\n",
      "Epoch 6 loss 99160.086\n",
      "Epoch 7 loss 98985.337\n",
      "Epoch 8 loss 99090.865\n",
      "Epoch 9 loss 98862.203\n",
      "Epoch 10 loss 98819.208\n",
      "Epoch 1 loss 98651.113\n",
      "Epoch 2 loss 98808.555\n",
      "Epoch 3 loss 98327.102\n",
      "Epoch 4 loss 98488.179\n",
      "Epoch 5 loss 99002.406\n",
      "Epoch 6 loss 98661.000\n",
      "Epoch 7 loss 98395.529\n",
      "Epoch 8 loss 98458.462\n",
      "Epoch 9 loss 98737.554\n",
      "Epoch 10 loss 98672.993\n"
     ]
    }
   ],
   "source": [
    "gd_model = GradientDescentMatrixFactorization(64, .01, N_USERS, N_ITEMS)\n",
    "gd_model.fit(user_item, 10, .1)\n",
    "gd_model.fit(user_item, 10, .01)\n",
    "gd_model.fit(user_item, 10, .001)\n",
    "gd_model.fit(user_item, 10, .0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1203    Godfather: Part II, The (1974)',\n",
       " '3266    Jail Bait (1954)',\n",
       " '761    Vie est belle, La (Life is Rosey) (1987)',\n",
       " '104    Nobody Loves Me (Keiner liebt mich) (1994)',\n",
       " '3526    Held Up (2000)',\n",
       " \"3748    Pot O' Gold (1941)\",\n",
       " '739    Man from Down Under, The (1943)',\n",
       " \"3713    Shaft's Big Score! (1972)\",\n",
       " '3583    City of the Living Dead (Paura nella cittа dei...',\n",
       " '2369    Outside Ozona (1998)',\n",
       " '3346    Mirror, The (Zerkalo) (1975)',\n",
       " '32    Wings of Courage (1995)',\n",
       " '3015    Home Page (1999)',\n",
       " '720    Institute Benjamenta, or This Dream People Cal...',\n",
       " '587    Tough and Deadly (1995)',\n",
       " '784    Midnight Dancers (Sibak) (1994)',\n",
       " '3391    Hillbillys in a Haunted House (1967)',\n",
       " '281    New York Cop (1996)',\n",
       " '109    Taxi Driver (1976)',\n",
       " '2818    Simon Sez (1999)']"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similars(858, gd_model)  # The Godfather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Не использую готовые решения, реализовать матричное разложение используя ALS на implicit данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from scipy.sparse.linalg import lsqr\n",
    "\n",
    "\n",
    "def solve_parameters(target_embeddings, target_biases, interactions_lists, \n",
    "                     fixed_embeddings, fixed_biases, dim, reg_alpha):\n",
    "    loss = 0.\n",
    "    for x, (fixed_indexes, targets) in interactions_lists.items():\n",
    "        a = np.hstack((\n",
    "            np.ones((len(fixed_indexes), 1)),\n",
    "            fixed_embeddings[fixed_indexes]\n",
    "        ))\n",
    "        b = targets - fixed_biases[fixed_indexes]\n",
    "        \n",
    "        a = np.vstack((a, np.zeros((dim, dim + 1))))\n",
    "        a[np.arange(dim) + len(fixed_indexes), np.arange(dim) + 1] = reg_alpha\n",
    "        b = np.concatenate((b, np.zeros(dim)))\n",
    "        \n",
    "        solution, *_ = np.linalg.lstsq(a, b, None)\n",
    "        target_biases[x] = solution[0]\n",
    "        target_embeddings[x] = solution[1:]\n",
    "        loss += np.sum((a @ solution - b) ** 2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class ALSMatrixFactorization(MatrixFactorizationBase):\n",
    "    def __init__(self, dim, reg_alpha, n_users, n_items):\n",
    "        super().__init__(dim, reg_alpha, n_users, n_items)\n",
    "    \n",
    "    def fit(self, interactions, n_iterations):\n",
    "        users_ids = interactions.row\n",
    "        items_ids = interactions.col\n",
    "        n_negatives = n_positives = interactions.nnz\n",
    "        \n",
    "        negative_users_ids = np.random.choice(np.unique(users_ids), n_negatives)\n",
    "        negative_items_ids = np.random.choice(np.unique(items_ids), n_negatives)\n",
    "        \n",
    "        users_int_lists = defaultdict(lambda: ([], []))\n",
    "        items_int_lists = defaultdict(lambda: ([], []))\n",
    "        for user_id, item_id, target in zip(np.concatenate((users_ids, negative_users_ids)), \n",
    "                                            np.concatenate((items_ids, negative_items_ids)),\n",
    "                                            np.concatenate((np.ones(n_positives), np.zeros(n_negatives)))):\n",
    "            user_items_ids, user_targets = users_int_lists[user_id]\n",
    "            user_items_ids.append(item_id)\n",
    "            user_targets.append(target)\n",
    "            item_users_ids, item_targets = items_int_lists[item_id]\n",
    "            item_users_ids.append(user_id)\n",
    "            item_targets.append(target)\n",
    "        users_int_lists = {user_id: (np.array(user_items_ids), np.array(user_targets))\n",
    "                           for user_id, (user_items_ids, user_targets) in users_int_lists.items()}\n",
    "        items_int_lists = {item_id: (np.array(item_users_ids), np.array(item_targets))\n",
    "                           for item_id, (item_users_ids, item_targets) in items_int_lists.items()}\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            users_loss = solve_parameters(self.users_embeddings, self.users_biases, users_int_lists, \n",
    "                                          self.items_embeddings, self.items_biases, self.dim, self.reg_param)\n",
    "            items_loss = solve_parameters(self.items_embeddings, self.items_biases, items_int_lists, \n",
    "                                          self.users_embeddings, self.users_biases, self.dim, self.reg_param)\n",
    "            print(f'Iteration {iteration + 1} loss {users_loss + items_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 loss 290660.530\n",
      "Iteration 2 loss 141956.850\n",
      "Iteration 3 loss 117941.981\n",
      "Iteration 4 loss 108628.399\n",
      "Iteration 5 loss 103560.520\n",
      "Iteration 6 loss 100334.521\n",
      "Iteration 7 loss 98084.057\n",
      "Iteration 8 loss 96414.384\n",
      "Iteration 9 loss 95118.509\n",
      "Iteration 10 loss 94078.219\n"
     ]
    }
   ],
   "source": [
    "als_model = ALSMatrixFactorization(64, 1, N_USERS, N_ITEMS)\n",
    "als_model.fit(user_item, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1023    Die Hard (1988)',\n",
       " '3633    Mad Max (1979)',\n",
       " '1196    Alien (1979)',\n",
       " '3458    Predator (1987)',\n",
       " '1182    Aliens (1986)',\n",
       " '2219    Thing, The (1982)',\n",
       " '3402    Close Encounters of the Third Kind (1977)',\n",
       " '1222    Glory (1989)',\n",
       " '1113    Escape from New York (1981)',\n",
       " '1885    Rocky (1976)',\n",
       " '1271    Indiana Jones and the Last Crusade (1989)',\n",
       " '2916    Robocop (1987)',\n",
       " '2458    Westworld (1973)',\n",
       " '1491    Fifth Element, The (1997)',\n",
       " '1355    Star Trek IV: The Voyage Home (1986)',\n",
       " '1353    Star Trek: The Wrath of Khan (1982)',\n",
       " '1952    Dune (1984)',\n",
       " '1188    Clockwork Orange, A (1971)',\n",
       " '2571    Superman (1978)',\n",
       " '2125    Untouchables, The (1987)']"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similars(1240, als_model)  # Terminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3. Не использую готовые решения, реализовать матричное разложение BPR на implicit данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPR_BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "BPR_MARGIN = 10\n",
    "\n",
    "\n",
    "class BPRMF(MatrixFactorizationBase):\n",
    "    def __init__(self, dim, reg_alpha, n_users, n_items):\n",
    "        super().__init__(dim, reg_alpha, n_users, n_items)\n",
    "        \n",
    "    def fit(self, interactions, n_epochs, lr):\n",
    "        users = interactions.row\n",
    "        positives = interactions.col\n",
    "            \n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            negatives = np.random.choice(np.unique(positives), len(interactions.data))\n",
    "            \n",
    "            loss = 0.\n",
    "            indexes = np.arange(interactions.nnz)\n",
    "            for batch_start in range(0, interactions.nnz, BPR_BATCH_SIZE):\n",
    "                batch_indexes = indexes[batch_start:batch_start + BPR_BATCH_SIZE]\n",
    "                batch_users = users[batch_indexes]\n",
    "                batch_positives = positives[batch_indexes]\n",
    "                batch_negatives = negatives[batch_indexes]\n",
    "                \n",
    "                items_embeddings_diff = self.items_embeddings[batch_positives] - self.items_embeddings[batch_negatives]\n",
    "                x_uij = scalar_prods(self.users_embeddings[batch_users], items_embeddings_diff) + \\\n",
    "                    self.items_biases[batch_positives] - self.items_biases[batch_negatives]\n",
    "                x_uij = np.maximum(x_uij, -100)\n",
    "                mask = x_uij < BPR_MARGIN\n",
    "                x_uij_negxp = np.exp(-np.minimum(x_uij, BPR_MARGIN))\n",
    "                loss += np.log((1 + x_uij_negxp)).sum()\n",
    "                loss += self.reg_param * (\n",
    "                    np.linalg.norm(self.users_embeddings[batch_users], axis=1).sum() + \n",
    "                    np.linalg.norm(self.items_embeddings[batch_positives], axis=1).sum() + \n",
    "                    np.linalg.norm(self.items_embeddings[batch_negatives], axis=1).sum())\n",
    "                loss_grads = -x_uij_negxp[mask] / (1 + x_uij_negxp[mask])\n",
    "                positive_biases_grads = loss_grads\n",
    "                negative_biases_grads = -loss_grads\n",
    "                loss_grads = loss_grads.reshape((-1, 1))\n",
    "                user_grads = loss_grads * items_embeddings_diff[mask] + \\\n",
    "                        self.reg_param * self.users_embeddings[batch_users][mask]\n",
    "                positive_grads = loss_grads * self.users_embeddings[batch_users][mask] + \\\n",
    "                        self.reg_param * self.items_embeddings[batch_positives][mask]\n",
    "                negative_grads = -loss_grads * self.users_embeddings[batch_users][mask] + \\\n",
    "                        self.reg_param * self.items_embeddings[batch_negatives][mask]\n",
    "                \n",
    "                np.add.at(self.users_embeddings, batch_users[mask], -lr * user_grads)\n",
    "                np.add.at(self.items_embeddings, batch_positives[mask], -lr * positive_grads)\n",
    "                np.add.at(self.items_embeddings, batch_negatives[mask], -lr * negative_grads)\n",
    "                np.add.at(self.items_biases, batch_positives[mask], -lr * positive_biases_grads)\n",
    "                np.add.at(self.items_biases, batch_negatives[mask], -lr * negative_biases_grads)\n",
    "            print(f'Epoch {epoch} loss {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss 299042.323\n",
      "Epoch 2 loss 240029.660\n",
      "Epoch 3 loss 225742.266\n",
      "Epoch 4 loss 219017.032\n",
      "Epoch 5 loss 214659.635\n",
      "Epoch 6 loss 211594.017\n",
      "Epoch 7 loss 209342.547\n",
      "Epoch 8 loss 206350.896\n",
      "Epoch 9 loss 205297.520\n",
      "Epoch 10 loss 202867.692\n",
      "Epoch 1 loss 201016.670\n",
      "Epoch 2 loss 200779.868\n",
      "Epoch 3 loss 200177.160\n",
      "Epoch 4 loss 199488.917\n",
      "Epoch 5 loss 199659.475\n",
      "Epoch 6 loss 199520.982\n",
      "Epoch 7 loss 199657.655\n",
      "Epoch 8 loss 199400.754\n",
      "Epoch 9 loss 198900.161\n",
      "Epoch 10 loss 198868.315\n"
     ]
    }
   ],
   "source": [
    "bpr_model = BPRMF(256, .01, N_USERS, N_ITEMS)\n",
    "bpr_model.fit(user_item, 10, .01)\n",
    "bpr_model.fit(user_item, 10, .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1545    G.I. Jane (1997)',\n",
       " \"61    Mr. Holland's Opus (1995)\",\n",
       " '3188    Bodyguard, The (1992)',\n",
       " '1732    U.S. Marshalls (1998)',\n",
       " '2655    Runaway Bride (1999)',\n",
       " \"1342    Preacher's Wife, The (1996)\",\n",
       " '1450    Saint, The (1997)',\n",
       " '289    Outbreak (1995)',\n",
       " '1358    Young Guns II (1990)',\n",
       " '1825    Six Days Seven Nights (1998)',\n",
       " '1848    Armageddon (1998)',\n",
       " '166    First Knight (1995)',\n",
       " '3181    Alive (1993)',\n",
       " '601    One Fine Day (1996)',\n",
       " '2427    Blast from the Past (1999)',\n",
       " '1823    Perfect Murder, A (1998)',\n",
       " '1014    Robin Hood: Prince of Thieves (1991)',\n",
       " '593    Pretty Woman (1990)',\n",
       " '795    Time to Kill, A (1996)',\n",
       " '778    Nutty Professor, The (1996)']"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similars(1721, bpr_model)  # Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4. Не использую готовые решения, реализовать матричное разложение WARP на implicit данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARP_BATCH_SIZE = 4\n",
    "WARP_MAX_SAMPLE_TRIALS = 1000\n",
    "WARP_MARGIN = 1\n",
    "\n",
    "\n",
    "def project_vectors(vectors, indexes, max_norm):\n",
    "    vector_norms = np.linalg.norm(vectors[indexes], axis=1)\n",
    "    vectors[indexes] *= np.maximum(max_norm / vector_norms, 1).reshape((-1, 1))\n",
    "\n",
    "\n",
    "class WARPMF(MatrixFactorizationBase):\n",
    "    def __init__(self, dim, reg_param, n_users, n_items):\n",
    "        super().__init__(dim, reg_param, n_users, n_items)\n",
    "        \n",
    "    def fit(self, interactions, n_epochs, lr):\n",
    "        users = interactions.row\n",
    "        positives = interactions.col\n",
    "        unique_items = np.unique(positives)\n",
    "            \n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            loss = 0.\n",
    "            indexes = np.arange(interactions.nnz)\n",
    "            for batch_start in range(0, interactions.nnz, WARP_BATCH_SIZE):\n",
    "                batch_indexes = indexes[batch_start:batch_start + WARP_BATCH_SIZE]\n",
    "                batch_users = users[batch_indexes]\n",
    "                batch_positives = positives[batch_indexes]\n",
    "                positives_similarities = self.similarities(batch_users, batch_positives)\n",
    "                \n",
    "                batch_negatives = np.random.choice(unique_items, len(batch_users))\n",
    "                negatives_similarities = self.similarities(batch_users, batch_negatives)\n",
    "                good_mask = positives_similarities - negatives_similarities > WARP_MARGIN\n",
    "                sampling_counters = np.ones(len(batch_users))\n",
    "                for _ in range(WARP_MAX_SAMPLE_TRIALS):\n",
    "                    n_good = good_mask.sum()\n",
    "                    if n_good == 0:\n",
    "                        break\n",
    "                    batch_negatives[good_mask] = np.random.choice(unique_items, n_good)\n",
    "                    sampling_counters[good_mask] += 1\n",
    "                    negatives_similarities[good_mask] = self.similarities(\n",
    "                        batch_users[good_mask], batch_negatives[good_mask])\n",
    "                    good_mask = positives_similarities - negatives_similarities > WARP_MARGIN\n",
    "                to_opt_mask = ~good_mask\n",
    "                n_to_opt = to_opt_mask.sum()\n",
    "                \n",
    "                batch_users = batch_users[to_opt_mask]\n",
    "                batch_positives = batch_positives[to_opt_mask]\n",
    "                batch_negatives = batch_negatives[to_opt_mask]\n",
    "                positives_similarities = positives_similarities[to_opt_mask]\n",
    "                negatives_similarities = negatives_similarities[to_opt_mask]\n",
    "                samples_weights = np.log((WARP_MAX_SAMPLE_TRIALS - 1) / sampling_counters[to_opt_mask])\n",
    "                \n",
    "                \n",
    "                loss += np.sum((WARP_MARGIN + negatives_similarities - positives_similarities) * samples_weights)\n",
    "                positive_biases_grads = -samples_weights\n",
    "                negative_biases_grads = samples_weights\n",
    "                samples_weights = np.expand_dims(samples_weights, 1)\n",
    "                user_grads = samples_weights * \\\n",
    "                        (self.items_embeddings[batch_negatives] - self.items_embeddings[batch_positives])\n",
    "                positive_grads = samples_weights * (-self.users_embeddings[batch_users])\n",
    "                negative_grads = samples_weights * self.users_embeddings[batch_users]\n",
    "                \n",
    "                np.add.at(self.users_embeddings, batch_users, -lr * user_grads)\n",
    "                np.add.at(self.items_embeddings, batch_positives, -lr * positive_grads)\n",
    "                np.add.at(self.items_embeddings, batch_negatives, -lr * negative_grads)\n",
    "                project_vectors(self.users_embeddings, batch_users, self.reg_param)\n",
    "                project_vectors(self.items_embeddings, batch_positives, self.reg_param)\n",
    "                project_vectors(self.items_embeddings, batch_negatives, self.reg_param)\n",
    "                np.add.at(self.items_biases, batch_positives, -lr * positive_biases_grads)\n",
    "                np.add.at(self.items_biases, batch_negatives, -lr * negative_biases_grads)\n",
    "            print(f'Epoch {epoch} loss {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss 3358708.315\n",
      "Epoch 2 loss 2668730.417\n",
      "Epoch 3 loss 2484464.265\n",
      "Epoch 4 loss 2475452.125\n",
      "Epoch 5 loss 2507519.921\n"
     ]
    }
   ],
   "source": [
    "warp_model = WARPMF(64, 1, N_USERS, N_ITEMS)\n",
    "warp_model.fit(user_item, 5, .0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3520    Kill, Baby... Kill! (Operazione Paura) (1966)',\n",
       " '704    Of Love and Shadows (1994)',\n",
       " '3668    Lonely Are the Brave (1962)',\n",
       " '743    Month by the Lake, A (1995)',\n",
       " '1277    Real Genius (1985)',\n",
       " '556    Beans of Egypt, Maine, The (1994)',\n",
       " '2700    Yards, The (1999)',\n",
       " '1548    Cop Land (1997)',\n",
       " '2273    Hard Core Logo (1996)',\n",
       " '1646    Amistad (1997)',\n",
       " \"3445    Joe Gould's Secret (2000)\",\n",
       " '2325    Prince of Egypt, The (1998)',\n",
       " '3257    What Planet Are You From? (2000)',\n",
       " '1658    Harlem River Drive (1996)',\n",
       " '1414    Meet Wally Sparks (1997)',\n",
       " '3032    Fatal Attraction (1987)',\n",
       " '2190    Blame It on Rio (1984)',\n",
       " 'Series([], )',\n",
       " '1178    Star Wars: Episode V - The Empire Strikes Back...',\n",
       " '730    Honigmond (1996)']"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similars(260, warp_model)  # Star wars a new hope"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
