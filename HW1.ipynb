{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Матричные факторизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе вам предстоит познакомиться с практической стороной матричных разложений.\n",
    "Работа поделена на 4 задания:\n",
    "1. Вам необходимо реализовать SVD разложения используя SGD на explicit данных\n",
    "2. Вам необходимо реализовать матричное разложения используя ALS на implicit данных\n",
    "3. Вам необходимо реализовать матричное разложения используя BPR(pair-wise loss) на implicit данных\n",
    "4. Вам необходимо реализовать матричное разложения используя WARP(list-wise loss) на implicit данных\n",
    "\n",
    "Мягкий дедлайн 28 Сентября (пишутся замечания, выставляется оценка, есть возможность исправить до жесткого дедлайна)\n",
    "\n",
    "Жесткий дедлайн 5 Октября (Итоговая проверка)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import implicit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# from lightfm.datasets import fetch_movielens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе мы будем работать с explicit датасетом movieLens, в котором представленны пары user_id movie_id и rating выставленный пользователем фильму\n",
    "\n",
    "Скачать датасет можно по ссылке https://grouplens.org/datasets/movielens/1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('ml-1m/ratings.dat', delimiter='::', header=None, \n",
    "        names=['user_id', 'movie_id', 'rating', 'timestamp'], \n",
    "        usecols=['user_id', 'movie_id', 'rating'], engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_info = pd.read_csv('ml-1m/movies.dat', delimiter='::', header=None, \n",
    "        names=['movie_id', 'name', 'category'], engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1197</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1287</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2804</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>594</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>919</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating\n",
       "0        1      1193       5\n",
       "1        1       661       3\n",
       "2        1       914       3\n",
       "3        1      3408       4\n",
       "4        1      2355       5\n",
       "5        1      1197       3\n",
       "6        1      1287       5\n",
       "7        1      2804       5\n",
       "8        1       594       4\n",
       "9        1       919       4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы преобразовать текущий датасет в Implicit, давайте считать что позитивная оценка это оценка >=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit_ratings = ratings.loc[(ratings['rating'] >= 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1287</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2804</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>594</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>919</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>595</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>938</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2398</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  movie_id  rating\n",
       "0         1      1193       5\n",
       "3         1      3408       4\n",
       "4         1      2355       5\n",
       "6         1      1287       5\n",
       "7         1      2804       5\n",
       "8         1       594       4\n",
       "9         1       919       4\n",
       "10        1       595       5\n",
       "11        1       938       4\n",
       "12        1      2398       4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit_ratings.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удобнее работать с sparse матричками, давайте преобразуем DataFrame в CSR матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = implicit_ratings[\"user_id\"]\n",
    "movies = implicit_ratings[\"movie_id\"]\n",
    "user_item = sp.coo_matrix((np.ones_like(users), (users, movies)))\n",
    "user_item_t_csr = user_item.T.tocsr()\n",
    "user_item_csr = user_item.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве примера воспользуемся ALS разложением из библиотеки implicit\n",
    "\n",
    "Зададим размерность латентного пространства равным 64, это же определяет размер user/item эмбедингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n"
     ]
    }
   ],
   "source": [
    "model = implicit.als.AlternatingLeastSquares(factors=64, iterations=100, calculate_training_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве loss здесь всеми любимый RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be7412045704a969770142e3762889e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(user_item_t_csr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим похожие фильмы по 1 movie_id = Истории игрушек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                                name                      category\n",
       "0         1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2         3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3         4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4         5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similars = lambda item_id, model : [movie_info[movie_info[\"movie_id\"] == x[0]][\"name\"].to_string() \n",
    "                                        for x in model.similar_items(item_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, симилары действительно оказались симиларами.\n",
    "\n",
    "Качество симиларов часто является хорошим способом проверить качество алгоритмов.\n",
    "\n",
    "P.S. Если хочется поглубже разобраться в том как разные алгоритмы формируют разные латентные пространства, рекомендую загружать полученные вектора в tensorBoard и смотреть на сформированное пространство"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0    Toy Story (1995)',\n",
       " '3045    Toy Story 2 (1999)',\n",
       " \"2286    Bug's Life, A (1998)\",\n",
       " '584    Aladdin (1992)',\n",
       " '33    Babe (1995)',\n",
       " '360    Lion King, The (1994)',\n",
       " '1526    Hercules (1997)',\n",
       " '2315    Babe: Pig in the City (1998)',\n",
       " '2618    Tarzan (1999)',\n",
       " '2252    Pleasantville (1998)']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similars(1, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь построим рекомендации для юзеров\n",
    "\n",
    "Как мы видим юзеру нравится фантастика, значит и в рекомендациях ожидаем увидеть фантастику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_user_history = lambda user_id, implicit_ratings : [movie_info[movie_info[\"movie_id\"] == x][\"name\"].to_string() \n",
    "                                            for x in implicit_ratings[implicit_ratings[\"user_id\"] == user_id][\"movie_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3399    Hustler, The (1961)',\n",
       " '2882    Fistful of Dollars, A (1964)',\n",
       " '1196    Alien (1979)',\n",
       " '1023    Die Hard (1988)',\n",
       " '257    Star Wars: Episode IV - A New Hope (1977)',\n",
       " '1959    Saving Private Ryan (1998)',\n",
       " '476    Jurassic Park (1993)',\n",
       " '1180    Raiders of the Lost Ark (1981)',\n",
       " '1885    Rocky (1976)',\n",
       " '1081    E.T. the Extra-Terrestrial (1982)',\n",
       " '3349    Thelma & Louise (1991)',\n",
       " '3633    Mad Max (1979)',\n",
       " '2297    King Kong (1933)',\n",
       " '1366    Jaws (1975)',\n",
       " '1183    Good, The Bad and The Ugly, The (1966)',\n",
       " '2623    Run Lola Run (Lola rennt) (1998)',\n",
       " '2878    Goldfinger (1964)',\n",
       " '1220    Terminator, The (1984)']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_user_history(4, implicit_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось! \n",
    "\n",
    "Мы действительно порекомендовали пользователю фантастику и боевики, более того встречаются продолжения тех фильмов, которые он высоко оценил"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations = lambda user_id, model : [movie_info[movie_info[\"movie_id\"] == x[0]][\"name\"].to_string() \n",
    "                                               for x in model.recommend(user_id, user_item_csr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['585    Terminator 2: Judgment Day (1991)',\n",
       " '2502    Matrix, The (1999)',\n",
       " '1284    Butch Cassidy and the Sundance Kid (1969)',\n",
       " '1271    Indiana Jones and the Last Crusade (1989)',\n",
       " '1178    Star Wars: Episode V - The Empire Strikes Back...',\n",
       " '1182    Aliens (1986)',\n",
       " '1884    French Connection, The (1971)',\n",
       " '1892    Rain Man (1988)',\n",
       " '957    African Queen, The (1951)',\n",
       " '847    Godfather, The (1972)']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations(4, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь ваша очередь реализовать самые популярные алгоритмы матричных разложений\n",
    "\n",
    "Что будет оцениваться:\n",
    "1. Корректность алгоритма\n",
    "2. Качество получившихся симиларов\n",
    "3. Качество итоговых рекомендаций для юзера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Не использую готовые решения, реализовать SVD разложение используя SGD на explicit данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_USERS = users.max() + 1\n",
    "N_ITEMS = movies.max() + 1\n",
    "\n",
    "\n",
    "def scalar_prods(vecs1, vecs2):\n",
    "    return np.sum(vecs1 * vecs2, axis=1).flatten()\n",
    "\n",
    "\n",
    "class MatrixFactorizationBase:\n",
    "    def __init__(self, dim, reg_param, n_users, n_items):\n",
    "        self.dim = dim\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        init_std = 1 / dim ** .5\n",
    "        self.users_embeddings = np.random.normal(0, init_std, (n_users, dim))\n",
    "        self.items_embeddings = np.random.normal(0, init_std, (n_items, dim))\n",
    "        self.users_biases = np.random.uniform(0, .5, n_users)\n",
    "        self.items_biases = np.random.uniform(0, .5, n_items)\n",
    "        self.reg_param = reg_param\n",
    "    \n",
    "    def fit(self, interactions, n_epochs, lr):\n",
    "        pass\n",
    "    \n",
    "    def similarities(self, users_ids, items_ids):\n",
    "        return self.users_biases[users_ids] + self.items_biases[items_ids] + \\\n",
    "                scalar_prods(self.users_embeddings[users_ids], self.items_embeddings[items_ids])\n",
    "    \n",
    "    def recommend(self, user_id, _ = None, n_recs = 20):\n",
    "        similarities = self.items_embeddings @ self.users_embeddings[user_id]\n",
    "        closest_item_ids = similarities.argsort()[::-1][:n_recs]\n",
    "        return list(zip(closest_item_ids, similarities[closest_item_ids]))\n",
    "    \n",
    "    def similar_items(self, item_id, n_items = 20):\n",
    "        similarities = self.items_embeddings @ self.items_embeddings[item_id]\n",
    "        items_by_similariry = similarities.argsort()[::-1]\n",
    "        items_by_similariry = items_by_similariry[items_by_similariry != item_id]\n",
    "        most_similar_items = items_by_similariry[:n_items]\n",
    "        return list(zip(most_similar_items, similarities[most_similar_items]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_BATCH_SIZE = 512\n",
    "\n",
    "\n",
    "class GradientDescentMatrixFactorization(MatrixFactorizationBase):\n",
    "    def __init__(self, dim, reg_alpha, n_users, n_items):\n",
    "        super().__init__(dim, reg_alpha, n_users, n_items)\n",
    "    \n",
    "    \n",
    "    def make_gd_step(self, users_ids, items_ids, targets, lr):\n",
    "        users_gradients = np.zeros_like(self.users_embeddings)\n",
    "        items_gradients = np.zeros_like(self.items_embeddings)\n",
    "        users_biases_gradients = np.zeros_like(self.users_biases)\n",
    "        items_biases_gradients = np.zeros_like(self.items_biases)\n",
    "        \n",
    "        predictions = self.similarities(users_ids, items_ids)\n",
    "        errors_gradients = np.expand_dims(2 * (predictions - targets), 1)\n",
    "        np.add.at(users_gradients, users_ids, errors_gradients * self.items_embeddings[items_ids])\n",
    "        np.add.at(items_gradients, items_ids, errors_gradients * self.users_embeddings[users_ids])\n",
    "        np.add.at(users_gradients, users_ids, 2 * self.reg_param * self.users_embeddings[users_ids])\n",
    "        np.add.at(items_gradients, items_ids, 2 * self.reg_param * self.items_embeddings[items_ids])\n",
    "        np.add.at(users_biases_gradients, users_ids, errors_gradients.flatten())\n",
    "        np.add.at(items_biases_gradients, items_ids, errors_gradients.flatten())\n",
    "        loss = np.sum((predictions - targets) ** 2) + \\\n",
    "               self.reg_param * (np.linalg.norm(self.users_embeddings[users_ids], axis=1).sum() + \\\n",
    "                                 np.linalg.norm(self.items_embeddings[items_ids], axis=1).sum())\n",
    "\n",
    "        self.users_embeddings -= lr * users_gradients\n",
    "        self.items_embeddings -= lr * items_gradients\n",
    "        self.users_biases -= lr * users_biases_gradients\n",
    "        self.items_biases -= lr * items_biases_gradients\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def fit(self, interactions, n_epochs, lr):\n",
    "        n_negatives = n_samples = len(interactions.data)\n",
    "        users_ids = interactions.row\n",
    "        items_ids = interactions.col\n",
    "        \n",
    "        unique_users = np.array(list(set(users_ids)))\n",
    "        unique_items = np.array(list(set(items_ids)))\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            neg_users = np.random.choice(self.n_users, n_negatives)\n",
    "            neg_items = np.random.choice(self.n_items, n_negatives)\n",
    "            all_users = np.concatenate((users_ids, neg_users))\n",
    "            all_items = np.concatenate((items_ids, neg_items))\n",
    "            targets = np.concatenate((np.ones(n_samples), np.zeros(n_negatives)))\n",
    "            indexes = np.arange(n_samples + n_negatives)\n",
    "            np.random.shuffle(indexes)\n",
    "            \n",
    "            loss = 0.\n",
    "            for batch_start in range(0, len(indexes), SGD_BATCH_SIZE):\n",
    "                batch_indexes = indexes[batch_start:batch_start + SGD_BATCH_SIZE]\n",
    "                loss += self.make_gd_step(all_users[batch_indexes], all_items[batch_indexes], \n",
    "                                          targets[batch_indexes], lr)\n",
    "            \n",
    "            print(f'Epoch {epoch + 1} loss {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss 207908.131\n",
      "Epoch 2 loss 170237.973\n",
      "Epoch 3 loss 161376.897\n",
      "Epoch 4 loss 158045.253\n",
      "Epoch 5 loss 157160.179\n",
      "Epoch 6 loss 156216.265\n",
      "Epoch 7 loss 155766.333\n",
      "Epoch 8 loss 155350.140\n",
      "Epoch 9 loss 155462.277\n",
      "Epoch 10 loss 154472.684\n",
      "Epoch 1 loss 120567.601\n",
      "Epoch 2 loss 112119.982\n",
      "Epoch 3 loss 109082.125\n",
      "Epoch 4 loss 106930.833\n",
      "Epoch 5 loss 105838.345\n",
      "Epoch 6 loss 104557.156\n",
      "Epoch 7 loss 103799.172\n",
      "Epoch 8 loss 103268.247\n",
      "Epoch 9 loss 102645.161\n",
      "Epoch 10 loss 102274.456\n",
      "Epoch 1 loss 100051.748\n",
      "Epoch 2 loss 99687.070\n",
      "Epoch 3 loss 99441.169\n",
      "Epoch 4 loss 99334.014\n",
      "Epoch 5 loss 98994.764\n",
      "Epoch 6 loss 98909.821\n",
      "Epoch 7 loss 99168.435\n",
      "Epoch 8 loss 98825.640\n",
      "Epoch 9 loss 99100.435\n",
      "Epoch 10 loss 98891.201\n"
     ]
    }
   ],
   "source": [
    "gd_model = GradientDescentMatrixFactorization(64, .01, N_USERS, N_ITEMS)\n",
    "gd_model.fit(user_item, 10, .1)\n",
    "gd_model.fit(user_item, 10, .01)\n",
    "gd_model.fit(user_item, 10, .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1203    Godfather: Part II, The (1974)',\n",
       " '3266    Jail Bait (1954)',\n",
       " '2263    Belly (1998)',\n",
       " '2626    Boys, The (1997)',\n",
       " '619    Condition Red (1995)',\n",
       " '3526    Held Up (2000)',\n",
       " '2818    Simon Sez (1999)',\n",
       " '1214    Boat, The (Das Boot) (1981)',\n",
       " '3138    Snows of Kilimanjaro, The (1952)',\n",
       " 'Series([], )',\n",
       " '3461    Smoking/No Smoking (1993)',\n",
       " '3052    Hitch-Hiker, The (1953)',\n",
       " '2837    Random Hearts (1999)',\n",
       " 'Series([], )',\n",
       " '1884    French Connection, The (1971)',\n",
       " '1295    Paris Was a Woman (1995)',\n",
       " '3321    Shanghai Surprise (1986)',\n",
       " '3872    Sorority House Massacre II (1990)',\n",
       " 'Series([], )',\n",
       " 'Series([], )']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similars(858, gd_model)  # The Godfather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2882    Fistful of Dollars, A (1964)',\n",
       " '1183    Good, The Bad and The Ugly, The (1966)',\n",
       " \"3713    Shaft's Big Score! (1972)\",\n",
       " '2878    Goldfinger (1964)',\n",
       " '1267    Ben-Hur (1959)',\n",
       " '2297    King Kong (1933)',\n",
       " '3712    Shaft in Africa (1973)',\n",
       " '2875    Dirty Dozen, The (1967)',\n",
       " '1885    Rocky (1976)',\n",
       " \"2561    Besieged (L' Assedio) (1998)\",\n",
       " '287    Once Were Warriors (1994)',\n",
       " '3634    Mad Max 2 (a.k.a. The Road Warrior) (1981)',\n",
       " '1884    French Connection, The (1971)',\n",
       " '3633    Mad Max (1979)',\n",
       " '2993    Longest Day, The (1962)',\n",
       " '3337    Captain Horatio Hornblower (1951)',\n",
       " '1950    Seven Samurai (The Magnificent Seven) (Shichin...',\n",
       " '3052    Hitch-Hiker, The (1953)',\n",
       " '1203    Godfather: Part II, The (1974)',\n",
       " '1213    Stalker (1979)']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations(4, gd_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Не использую готовые решения, реализовать матричное разложение используя ALS на implicit данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from scipy.sparse.linalg import lsqr\n",
    "\n",
    "\n",
    "def solve_parameters(target_embeddings, target_biases, interactions_lists, \n",
    "                     fixed_embeddings, fixed_biases, dim, reg_alpha):\n",
    "    loss = 0.\n",
    "    for x, (fixed_indexes, targets) in interactions_lists.items():\n",
    "        a = np.hstack((\n",
    "            np.ones((len(fixed_indexes), 1)),\n",
    "            fixed_embeddings[fixed_indexes]\n",
    "        ))\n",
    "        b = targets - fixed_biases[fixed_indexes]\n",
    "        \n",
    "        a = np.vstack((a, np.zeros((dim, dim + 1))))\n",
    "        a[np.arange(dim) + len(fixed_indexes), np.arange(dim) + 1] = reg_alpha\n",
    "        b = np.concatenate((b, np.zeros(dim)))\n",
    "        \n",
    "        solution, *_ = np.linalg.lstsq(a, b, None)\n",
    "        target_biases[x] = solution[0]\n",
    "        target_embeddings[x] = solution[1:]\n",
    "        loss += np.sum((a @ solution - b) ** 2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class ALSMatrixFactorization(MatrixFactorizationBase):\n",
    "    def __init__(self, dim, reg_alpha, n_users, n_items):\n",
    "        super().__init__(dim, reg_alpha, n_users, n_items)\n",
    "    \n",
    "    def fit(self, interactions, n_iterations):\n",
    "        users_ids = interactions.row\n",
    "        items_ids = interactions.col\n",
    "        n_negatives = n_positives = interactions.nnz\n",
    "        \n",
    "        negative_users_ids = np.random.choice(np.unique(users_ids), n_negatives)\n",
    "        negative_items_ids = np.random.choice(np.unique(items_ids), n_negatives)\n",
    "        \n",
    "        users_int_lists = defaultdict(lambda: ([], []))\n",
    "        items_int_lists = defaultdict(lambda: ([], []))\n",
    "        for user_id, item_id, target in zip(np.concatenate((users_ids, negative_users_ids)), \n",
    "                                            np.concatenate((items_ids, negative_items_ids)),\n",
    "                                            np.concatenate((np.ones(n_positives), np.zeros(n_negatives)))):\n",
    "            user_items_ids, user_targets = users_int_lists[user_id]\n",
    "            user_items_ids.append(item_id)\n",
    "            user_targets.append(target)\n",
    "            item_users_ids, item_targets = items_int_lists[item_id]\n",
    "            item_users_ids.append(user_id)\n",
    "            item_targets.append(target)\n",
    "        users_int_lists = {user_id: (np.array(user_items_ids), np.array(user_targets))\n",
    "                           for user_id, (user_items_ids, user_targets) in users_int_lists.items()}\n",
    "        items_int_lists = {item_id: (np.array(item_users_ids), np.array(item_targets))\n",
    "                           for item_id, (item_users_ids, item_targets) in items_int_lists.items()}\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            users_loss = solve_parameters(self.users_embeddings, self.users_biases, users_int_lists, \n",
    "                                          self.items_embeddings, self.items_biases, self.dim, self.reg_param)\n",
    "            items_loss = solve_parameters(self.items_embeddings, self.items_biases, items_int_lists, \n",
    "                                          self.users_embeddings, self.users_biases, self.dim, self.reg_param)\n",
    "            print(f'Iteration {iteration + 1} loss {users_loss + items_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 loss 293967.764\n",
      "Iteration 2 loss 141868.130\n",
      "Iteration 3 loss 117315.682\n",
      "Iteration 4 loss 107854.125\n",
      "Iteration 5 loss 102747.170\n",
      "Iteration 6 loss 99524.000\n",
      "Iteration 7 loss 97291.343\n",
      "Iteration 8 loss 95645.545\n",
      "Iteration 9 loss 94377.421\n",
      "Iteration 10 loss 93367.293\n"
     ]
    }
   ],
   "source": [
    "als_model = ALSMatrixFactorization(64, 1, N_USERS, N_ITEMS)\n",
    "als_model.fit(user_item, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2460    Planet of the Apes (1968)',\n",
       " '3458    Predator (1987)',\n",
       " '3634    Mad Max 2 (a.k.a. The Road Warrior) (1981)',\n",
       " '3633    Mad Max (1979)',\n",
       " '1023    Die Hard (1988)',\n",
       " '1182    Aliens (1986)',\n",
       " '1271    Indiana Jones and the Last Crusade (1989)',\n",
       " '1183    Good, The Bad and The Ugly, The (1966)',\n",
       " '2916    Robocop (1987)',\n",
       " '3402    Close Encounters of the Third Kind (1977)',\n",
       " '1254    Akira (1988)',\n",
       " '3615    Fabulous Baker Boys, The (1989)',\n",
       " '1196    Alien (1979)',\n",
       " '3443    Return to Me (2000)',\n",
       " '1255    Highlander (1986)',\n",
       " '3507    Hidden, The (1987)',\n",
       " '537    Blade Runner (1982)',\n",
       " '2847    Total Recall (1990)',\n",
       " '1353    Star Trek: The Wrath of Khan (1982)',\n",
       " '1204    Full Metal Jacket (1987)']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similars(1240, als_model)  # Terminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2358    Thin Red Line, The (1998)',\n",
       " '1183    Good, The Bad and The Ugly, The (1966)',\n",
       " '159    Crimson Tide (1995)',\n",
       " '3444    Rules of Engagement (2000)',\n",
       " '2638    Arlington Road (1999)',\n",
       " '2882    Fistful of Dollars, A (1964)',\n",
       " '2593    War of the Worlds, The (1953)',\n",
       " '2297    King Kong (1933)',\n",
       " '3129    Papillon (1973)',\n",
       " '1242    Great Escape, The (1963)',\n",
       " '1831    Children of Heaven, The (Bacheha-Ye Aseman) (1...',\n",
       " '3633    Mad Max (1979)',\n",
       " '1283    Man Who Would Be King, The (1975)',\n",
       " '1928    Exorcist, The (1973)',\n",
       " '345    Clear and Present Danger (1994)',\n",
       " '1556    Conspiracy Theory (1997)',\n",
       " '2322    Simple Plan, A (1998)',\n",
       " '2298    King Kong (1976)',\n",
       " '1284    Butch Cassidy and the Sundance Kid (1969)',\n",
       " '2260    American History X (1998)']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations(4, als_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3. Не использую готовые решения, реализовать матричное разложение BPR на implicit данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler:\n",
    "    def __init__(self, interactions, n_items):\n",
    "        self.positives = sp.csr_matrix(interactions)\n",
    "        self.n_items = n_items\n",
    "        self.items = np.unique(interactions.col)\n",
    "        \n",
    "    def get_positive_mask(self, samples, users):\n",
    "        return np.array(self.positives[users, samples], np.bool).ravel()\n",
    "        \n",
    "    def sample(self, users):\n",
    "        samples = np.random.choice(self.items, users.shape)\n",
    "        positive_mask = self.get_positive_mask(samples, users)\n",
    "        while np.any(positive_mask):\n",
    "            samples[positive_mask] = np.random.choice(self.items, positive_mask.sum())\n",
    "            positive_mask = self.get_positive_mask(samples, users)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPR_BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "BPR_MARGIN = 10\n",
    "\n",
    "\n",
    "class BPRMF(MatrixFactorizationBase):\n",
    "    def __init__(self, dim, reg_alpha, n_users, n_items):\n",
    "        super().__init__(dim, reg_alpha, n_users, n_items)\n",
    "        self.items_biases.fill(0.)\n",
    "        \n",
    "    def fit(self, interactions, n_epochs, lr):\n",
    "        users = interactions.row\n",
    "        positives = interactions.col\n",
    "        neg_sampler = NegativeSampler(interactions, self.n_items)\n",
    "        \n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            negatives = neg_sampler.sample(users)\n",
    "            \n",
    "            loss = 0.\n",
    "            indexes = np.arange(interactions.nnz)\n",
    "            for batch_start in range(0, interactions.nnz, BPR_BATCH_SIZE):\n",
    "                batch_indexes = indexes[batch_start:batch_start + BPR_BATCH_SIZE]\n",
    "                batch_users = users[batch_indexes]\n",
    "                batch_positives = positives[batch_indexes]\n",
    "                batch_negatives = negatives[batch_indexes]\n",
    "                \n",
    "                items_embeddings_diff = self.items_embeddings[batch_positives] - self.items_embeddings[batch_negatives]\n",
    "                x_uij = scalar_prods(self.users_embeddings[batch_users], items_embeddings_diff) + \\\n",
    "                    self.items_biases[batch_positives] - self.items_biases[batch_negatives]\n",
    "                x_uij = np.maximum(x_uij, -100)\n",
    "                mask = x_uij < BPR_MARGIN\n",
    "                x_uij_negxp = np.exp(-np.minimum(x_uij, BPR_MARGIN))\n",
    "                loss += np.log((1 + x_uij_negxp)).sum()\n",
    "                loss += self.reg_param * (\n",
    "                    np.linalg.norm(self.users_embeddings[batch_users], axis=1).sum() + \n",
    "                    np.linalg.norm(self.items_embeddings[batch_positives], axis=1).sum() + \n",
    "                    np.linalg.norm(self.items_embeddings[batch_negatives], axis=1).sum())\n",
    "                loss_grads = -x_uij_negxp[mask] / (1 + x_uij_negxp[mask])\n",
    "                positive_biases_grads = loss_grads\n",
    "                negative_biases_grads = -loss_grads\n",
    "                loss_grads = loss_grads.reshape((-1, 1))\n",
    "                user_grads = loss_grads * items_embeddings_diff[mask] + \\\n",
    "                        self.reg_param * self.users_embeddings[batch_users][mask]\n",
    "                positive_grads = loss_grads * self.users_embeddings[batch_users][mask] + \\\n",
    "                        self.reg_param * self.items_embeddings[batch_positives][mask]\n",
    "                negative_grads = -loss_grads * self.users_embeddings[batch_users][mask] + \\\n",
    "                        self.reg_param * self.items_embeddings[batch_negatives][mask]\n",
    "                \n",
    "                np.add.at(self.users_embeddings, batch_users[mask], -lr * user_grads)\n",
    "                np.add.at(self.items_embeddings, batch_positives[mask], -lr * positive_grads)\n",
    "                np.add.at(self.items_embeddings, batch_negatives[mask], -lr * negative_grads)\n",
    "                np.add.at(self.items_biases, batch_positives[mask], -lr * positive_biases_grads)\n",
    "                np.add.at(self.items_biases, batch_negatives[mask], -lr * negative_biases_grads)\n",
    "            print(f'Epoch {epoch} loss {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss 188855.444\n",
      "Epoch 2 loss 140422.260\n",
      "Epoch 3 loss 115396.494\n",
      "Epoch 4 loss 100430.471\n",
      "Epoch 5 loss 92118.844\n",
      "Epoch 6 loss 86271.426\n",
      "Epoch 7 loss 82004.376\n",
      "Epoch 8 loss 79540.138\n",
      "Epoch 9 loss 77162.847\n",
      "Epoch 10 loss 75755.202\n",
      "Epoch 11 loss 74510.076\n",
      "Epoch 12 loss 73166.029\n",
      "Epoch 13 loss 72722.278\n",
      "Epoch 14 loss 72446.959\n",
      "Epoch 15 loss 71974.235\n",
      "Epoch 16 loss 71383.896\n",
      "Epoch 17 loss 71466.016\n",
      "Epoch 18 loss 69847.746\n",
      "Epoch 19 loss 70776.888\n",
      "Epoch 20 loss 70664.603\n",
      "Epoch 1 loss 64545.749\n",
      "Epoch 2 loss 61588.601\n",
      "Epoch 3 loss 58764.479\n",
      "Epoch 4 loss 57613.898\n",
      "Epoch 5 loss 55730.433\n",
      "Epoch 6 loss 54708.877\n",
      "Epoch 7 loss 53523.619\n",
      "Epoch 8 loss 52444.860\n",
      "Epoch 9 loss 51272.799\n",
      "Epoch 10 loss 51343.805\n",
      "Epoch 11 loss 50374.642\n",
      "Epoch 12 loss 49933.907\n",
      "Epoch 13 loss 49174.655\n",
      "Epoch 14 loss 48687.495\n",
      "Epoch 15 loss 48545.366\n",
      "Epoch 16 loss 48385.429\n",
      "Epoch 17 loss 48035.137\n",
      "Epoch 18 loss 47248.350\n",
      "Epoch 19 loss 46712.613\n",
      "Epoch 20 loss 46939.643\n"
     ]
    }
   ],
   "source": [
    "bpr_model = BPRMF(64, .001, N_USERS, N_ITEMS)\n",
    "bpr_model.fit(user_item, 20, .1)\n",
    "bpr_model.fit(user_item, 20, .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2428    Message in a Bottle (1999)',\n",
       " \"61    Mr. Holland's Opus (1995)\",\n",
       " '3190    Far and Away (1992)',\n",
       " '3188    Bodyguard, The (1992)',\n",
       " '601    One Fine Day (1996)',\n",
       " '357    It Could Happen to You (1994)',\n",
       " '205    Walk in the Clouds, A (1995)',\n",
       " '2200    Indecent Proposal (1993)',\n",
       " '1332    Mirror Has Two Faces, The (1996)',\n",
       " '45    How to Make an American Quilt (1995)',\n",
       " '1678    Horse Whisperer, The (1998)',\n",
       " '3459    Prince of Tides, The (1991)',\n",
       " '520    Rudy (1993)',\n",
       " '2602    Notting Hill (1999)',\n",
       " '6    Sabrina (1995)',\n",
       " '47    Pocahontas (1995)',\n",
       " '2655    Runaway Bride (1999)',\n",
       " '267    Love Affair (1994)',\n",
       " '246    Immortal Beloved (1994)',\n",
       " '103    Bridges of Madison County, The (1995)']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similars(1721, bpr_model)  # Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2882    Fistful of Dollars, A (1964)',\n",
       " '3612    For a Few Dollars More (1965)',\n",
       " '1183    Good, The Bad and The Ugly, The (1966)',\n",
       " \"2853    Hang 'em High (1967)\",\n",
       " '2852    High Plains Drifter (1972)',\n",
       " '595    Wild Bunch, The (1969)',\n",
       " '2880    Dr. No (1962)',\n",
       " '3297    Where Eagles Dare (1969)',\n",
       " '3439    Outlaw Josey Wales, The (1976)',\n",
       " '2922    Live and Let Die (1973)',\n",
       " '2875    Dirty Dozen, The (1967)',\n",
       " '3450    Force 10 from Navarone (1978)',\n",
       " '2878    Goldfinger (1964)',\n",
       " '3702    Golden Voyage of Sinbad, The (1974)',\n",
       " '2332    Pale Rider (1985)',\n",
       " '2924    Thunderball (1965)',\n",
       " '1267    Ben-Hur (1959)',\n",
       " '1191    Once Upon a Time in the West (1969)',\n",
       " '1885    Rocky (1976)',\n",
       " '3633    Mad Max (1979)']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations(4, bpr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4. Не использую готовые решения, реализовать матричное разложение WARP на implicit данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARP_BATCH_SIZE = 4\n",
    "WARP_MAX_SAMPLE_TRIALS = 100\n",
    "WARP_MARGIN = 1\n",
    "\n",
    "\n",
    "def project_vectors(vectors, indexes, max_norm):\n",
    "    vector_norms = np.linalg.norm(vectors[indexes], axis=1)\n",
    "    vectors[indexes] *= np.maximum(max_norm / vector_norms, 1).reshape((-1, 1))\n",
    "\n",
    "\n",
    "class WARPMF(MatrixFactorizationBase):\n",
    "    def __init__(self, dim, reg_param, n_users, n_items):\n",
    "        super().__init__(dim, reg_param, n_users, n_items)\n",
    "        self.items_biases.fill(0.)\n",
    "        \n",
    "    def fit(self, interactions, n_epochs, lr):\n",
    "        users = interactions.row\n",
    "        positives = interactions.col\n",
    "        neg_sampler = NegativeSampler(interactions, self.n_items)\n",
    "            \n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            loss = 0.\n",
    "            indexes = np.arange(interactions.nnz)\n",
    "            for batch_start in range(0, interactions.nnz, WARP_BATCH_SIZE):\n",
    "                batch_indexes = indexes[batch_start:batch_start + WARP_BATCH_SIZE]\n",
    "                batch_users = users[batch_indexes]\n",
    "                batch_positives = positives[batch_indexes]\n",
    "                positives_similarities = self.similarities(batch_users, batch_positives)\n",
    "                \n",
    "                batch_negatives = neg_sampler.sample(batch_users)\n",
    "                negatives_similarities = self.similarities(batch_users, batch_negatives)\n",
    "                good_mask = positives_similarities - negatives_similarities > WARP_MARGIN\n",
    "                sampling_counters = np.ones(len(batch_users))\n",
    "                for _ in range(WARP_MAX_SAMPLE_TRIALS):\n",
    "                    n_good = good_mask.sum()\n",
    "                    if n_good == 0:\n",
    "                        break\n",
    "                    batch_negatives[good_mask] = neg_sampler.sample(batch_users[good_mask])\n",
    "                    sampling_counters[good_mask] += 1\n",
    "                    negatives_similarities[good_mask] = self.similarities(\n",
    "                        batch_users[good_mask], batch_negatives[good_mask])\n",
    "                    good_mask = positives_similarities - negatives_similarities > WARP_MARGIN\n",
    "                to_opt_mask = ~good_mask\n",
    "                n_to_opt = to_opt_mask.sum()\n",
    "                \n",
    "                batch_users = batch_users[to_opt_mask]\n",
    "                batch_positives = batch_positives[to_opt_mask]\n",
    "                batch_negatives = batch_negatives[to_opt_mask]\n",
    "                positives_similarities = positives_similarities[to_opt_mask]\n",
    "                negatives_similarities = negatives_similarities[to_opt_mask]\n",
    "                samples_weights = np.log((WARP_MAX_SAMPLE_TRIALS - 1) / sampling_counters[to_opt_mask])\n",
    "                \n",
    "                \n",
    "                loss += np.sum((WARP_MARGIN + negatives_similarities - positives_similarities) * samples_weights)\n",
    "                positive_biases_grads = -samples_weights\n",
    "                negative_biases_grads = samples_weights\n",
    "                samples_weights = np.expand_dims(samples_weights, 1)\n",
    "                user_grads = samples_weights * \\\n",
    "                        (self.items_embeddings[batch_negatives] - self.items_embeddings[batch_positives])\n",
    "                positive_grads = samples_weights * (-self.users_embeddings[batch_users])\n",
    "                negative_grads = samples_weights * self.users_embeddings[batch_users]\n",
    "                \n",
    "                np.add.at(self.users_embeddings, batch_users, -lr * user_grads)\n",
    "                np.add.at(self.items_embeddings, batch_positives, -lr * positive_grads)\n",
    "                np.add.at(self.items_embeddings, batch_negatives, -lr * negative_grads)\n",
    "                project_vectors(self.users_embeddings, batch_users, self.reg_param)\n",
    "                project_vectors(self.items_embeddings, batch_positives, self.reg_param)\n",
    "                project_vectors(self.items_embeddings, batch_negatives, self.reg_param)\n",
    "                np.add.at(self.items_biases, batch_positives, -lr * positive_biases_grads)\n",
    "                np.add.at(self.items_biases, batch_negatives, -lr * negative_biases_grads)\n",
    "            print(f'Epoch {epoch} loss {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss 3143885.894\n",
      "Epoch 2 loss 2362230.225\n",
      "Epoch 3 loss 1974496.895\n",
      "Epoch 4 loss 1765382.911\n",
      "Epoch 5 loss 1639075.072\n"
     ]
    }
   ],
   "source": [
    "warp_model = WARPMF(64, 4, N_USERS, N_ITEMS)\n",
    "warp_model.fit(user_item, 5 , .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1132    Wrong Trousers, The (1993)',\n",
       " '3045    Toy Story 2 (1999)',\n",
       " '591    Beauty and the Beast (1991)',\n",
       " '584    Aladdin (1992)',\n",
       " '1838    Mulan (1998)',\n",
       " '773    Hunchback of Notre Dame, The (1996)',\n",
       " '1526    Hercules (1997)',\n",
       " '360    Lion King, The (1994)',\n",
       " \"2286    Bug's Life, A (1998)\",\n",
       " \"3184    Wayne's World (1992)\",\n",
       " '33    Babe (1995)',\n",
       " '1205    Grand Day Out, A (1992)',\n",
       " '735    Close Shave, A (1995)',\n",
       " '2315    Babe: Pig in the City (1998)',\n",
       " '2290    Waking Ned Devine (1998)',\n",
       " '2502    Matrix, The (1999)',\n",
       " '2225    Antz (1998)',\n",
       " '2196    Nothing But Trouble (1991)',\n",
       " '2618    Tarzan (1999)',\n",
       " '1468    Grosse Pointe Blank (1997)']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similars(1, warp_model)  # Toy story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1178    Star Wars: Episode V - The Empire Strikes Back...',\n",
       " '1271    Indiana Jones and the Last Crusade (1989)',\n",
       " '1180    Raiders of the Lost Ark (1981)',\n",
       " '1192    Star Wars: Episode VI - Return of the Jedi (1983)',\n",
       " '1354    Star Trek III: The Search for Spock (1984)',\n",
       " '2571    Superman (1978)',\n",
       " \"941    It's a Wonderful Life (1946)\",\n",
       " '907    Wizard of Oz, The (1939)',\n",
       " '2046    Indiana Jones and the Temple of Doom (1984)',\n",
       " '2047    Lord of the Rings, The (1978)',\n",
       " '1353    Star Trek: The Wrath of Khan (1982)',\n",
       " '928    Adventures of Robin Hood, The (1938)',\n",
       " '2572    Superman II (1980)',\n",
       " '1220    Terminator, The (1984)',\n",
       " '1023    Die Hard (1988)',\n",
       " '942    Mr. Smith Goes to Washington (1939)',\n",
       " '1242    Great Escape, The (1963)',\n",
       " '1120    Monty Python and the Holy Grail (1974)',\n",
       " '1196    Alien (1979)',\n",
       " '3402    Close Encounters of the Third Kind (1977)']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similars(260, warp_model)  # Star wars a new hope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['929    Mark of Zorro, The (1940)',\n",
       " '2297    King Kong (1933)',\n",
       " '2993    Longest Day, The (1962)',\n",
       " '2660    Lolita (1962)',\n",
       " '2294    Godzilla (Gojira) (1954)',\n",
       " '3084    7th Voyage of Sinbad, The (1958)',\n",
       " '1263    High Noon (1952)',\n",
       " '2882    Fistful of Dollars, A (1964)',\n",
       " '2458    Westworld (1973)',\n",
       " '1583    Fire Down Below (1997)',\n",
       " '1317    Body Snatcher, The (1945)',\n",
       " '1789    Mr. Nice Guy (1997)',\n",
       " '1319    Bride of Frankenstein (1935)',\n",
       " '3667    Big Carnival, The (1951)',\n",
       " '2582    Frankenstein Meets the Wolf Man (1943)',\n",
       " '3295    Asphalt Jungle, The (1950)',\n",
       " '3559    Flying Tigers (1942)',\n",
       " \"2830    Gulliver's Travels (1939)\",\n",
       " '3215    They Might Be Giants (1971)',\n",
       " \"2063    Who's Afraid of Virginia Woolf? (1966)\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations(4, warp_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
